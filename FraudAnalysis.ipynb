{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQhn6lhf52HB391p0gEOmH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramme121/project/blob/main/FraudAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7YfTXogYNxa"
      },
      "outputs": [],
      "source": [
        "pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.types import IntegerType\n"
      ],
      "metadata": {
        "id": "7IiF9rw5Ynkr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "88PPGGkNYrSJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zUeXatA3ZD88"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Excel data from GitHub URL\n",
        "excel_file_url = \"https://github.com/Ramme121/project/raw/main/FraudData.xlsx\"\n",
        "df_pandas = pd.read_excel(excel_file_url)\n",
        "\n",
        "# Convert the Pandas DataFrame to a PySpark DataFrame\n",
        "df = spark.createDataFrame(df_pandas)\n"
      ],
      "metadata": {
        "id": "0W_m13itauTY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "# For demonstration purposes, let's fill missing values in \"TotalCredit\" with the mean value,\n",
        "# and convert the \"AnnualIncome\" column to IntegerType.\n",
        "\n",
        "# Filling missing values in the \"TotalCredit\" column with the mean value\n",
        "mean_total_credit = df.select(\"TotalCredit\").agg({\"TotalCredit\": \"mean\"}).collect()[0][0]\n",
        "df = df.na.fill(mean_total_credit, subset=[\"TotalCredit\"])\n",
        "\n",
        "# Converting the \"AnnualIncome\" column to IntegerType\n",
        "df = df.withColumn(\"AnnualIncome\", df[\"AnnualIncome\"].cast(IntegerType()))\n",
        "\n",
        "# Feature Selection (Assuming you want to use only selected columns for the model)\n",
        "selected_columns = [\"TotalCredit\", \"MonthlyPayments\", \"CreditScore\", \"CustomerAge\", \"Gender\", \"OwnsCar\", \"OwnsProperty\", \"NumberOfChildren\", \"Fraud\"]\n",
        "df = df.select(*selected_columns)\n",
        "\n",
        "# Check DataFrame Schema\n",
        "df.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTm6r76Fbq0z",
        "outputId": "0ffa06bc-e2d1-44c1-8717-bb46d90cfd92"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- TotalCredit: double (nullable = false)\n",
            " |-- MonthlyPayments: double (nullable = true)\n",
            " |-- CreditScore: long (nullable = true)\n",
            " |-- CustomerAge: long (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- OwnsCar: long (nullable = true)\n",
            " |-- OwnsProperty: long (nullable = true)\n",
            " |-- NumberOfChildren: long (nullable = true)\n",
            " |-- Fraud: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you already have the DataFrame 'df' loaded and preprocessed\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Converting 'CreditScore' and 'CustomerAge' columns to IntegerType\n",
        "df = df.withColumn(\"CreditScore\", df[\"CreditScore\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"CustomerAge\", df[\"CustomerAge\"].cast(IntegerType()))\n",
        "\n",
        "# Verify the updated DataFrame Schema\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRy9Qk2-d7Xc",
        "outputId": "1ce55d73-73f0-44c5-d3bc-4c226a202023"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- TotalCredit: double (nullable = false)\n",
            " |-- MonthlyPayments: double (nullable = true)\n",
            " |-- CreditScore: integer (nullable = true)\n",
            " |-- CustomerAge: integer (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- OwnsCar: long (nullable = true)\n",
            " |-- OwnsProperty: long (nullable = true)\n",
            " |-- NumberOfChildren: long (nullable = true)\n",
            " |-- Fraud: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#you already have the DataFrame 'df' loaded and preprocessed\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Converting 'OwnsCar', 'OwnsProperty', and 'NumberOfChildren' columns to IntegerType\n",
        "df = df.withColumn(\"OwnsCar\", df[\"OwnsCar\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"OwnsProperty\", df[\"OwnsProperty\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"NumberOfChildren\", df[\"NumberOfChildren\"].cast(IntegerType()))\n",
        "\n",
        "# Verify the updated DataFrame Schema\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yMPlimWehqy",
        "outputId": "9f1352c1-2407-4b03-90ac-2e9a4b6cb59c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- TotalCredit: double (nullable = false)\n",
            " |-- MonthlyPayments: double (nullable = true)\n",
            " |-- CreditScore: integer (nullable = true)\n",
            " |-- CustomerAge: integer (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- OwnsCar: integer (nullable = true)\n",
            " |-- OwnsProperty: integer (nullable = true)\n",
            " |-- NumberOfChildren: integer (nullable = true)\n",
            " |-- Fraud: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection (Assuming you want to use only selected columns for the model)\n",
        "selected_columns = [\"TotalCredit\", \"MonthlyPayments\", \"CreditScore\", \"CustomerAge\", \"Gender\", \"OwnsCar\", \"OwnsProperty\", \"NumberOfChildren\", \"Fraud\"]\n",
        "df = df.select(*selected_columns)\n",
        "\n",
        "# Check DataFrame Schema\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BMyd_NHfKox",
        "outputId": "cb72eed0-1eb1-42fa-ffa9-0b6e8df6583f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- TotalCredit: double (nullable = false)\n",
            " |-- MonthlyPayments: double (nullable = true)\n",
            " |-- CreditScore: integer (nullable = true)\n",
            " |-- CustomerAge: integer (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- OwnsCar: integer (nullable = true)\n",
            " |-- OwnsProperty: integer (nullable = true)\n",
            " |-- NumberOfChildren: integer (nullable = true)\n",
            " |-- Fraud: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa4aaYqWh6GO",
        "outputId": "0297666d-5287-423a-9d7f-963d505fb12d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "kUlX4VEph-zV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n",
        "\n",
        "# Replace \"GIT_REPOSITORY_URL\" with the actual URL of your Git repository\n",
        "excel_file_url = \"https://github.com/Ramme121/project/raw/main/FraudData.xlsx\"\n",
        "\n",
        "# Load the Excel file into a pandas DataFrame\n",
        "data_pd = pd.read_excel(excel_file_url)\n",
        "\n",
        "# Convert the pandas DataFrame to a Spark DataFrame\n",
        "data = spark.createDataFrame(data_pd)\n",
        "\n",
        "# Select relevant columns for the model\n",
        "selected_cols = [\"TotalCredit\", \"MonthlyPayments\", \"CreditScore\", \"CustomerAge\", \"Gender\", \"OwnsCar\", \"OwnsProperty\", \"NumberOfChildren\", \"Fraud\"]\n",
        "data = data.select(selected_cols)\n",
        "\n",
        "# Drop rows with any missing values\n",
        "data = data.na.drop()\n",
        "\n",
        "# VectorAssembler to combine features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=[\"TotalCredit\", \"MonthlyPayments\", \"CreditScore\", \"CustomerAge\", \"OwnsCar\", \"OwnsProperty\", \"NumberOfChildren\"], outputCol=\"features\")\n",
        "\n",
        "# Logistic Regression model\n",
        "lr = LogisticRegression(labelCol=\"Fraud\", featuresCol=\"features\")\n",
        "\n",
        "# Pipeline for assembling features and training the model\n",
        "pipeline = Pipeline(stages=[assembler, lr])\n",
        "\n",
        "# Split data into training and test sets\n",
        "training_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "# Train the model using the training data\n",
        "model = pipeline.fit(training_data)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Display the predictions\n",
        "predictions.select(\"Fraud\", \"prediction\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKxc6-ZaiCZ0",
        "outputId": "95cbe632-b55c-4439-f612-bd79a1daf15c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|Fraud|prediction|\n",
            "+-----+----------+\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    1|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "# Create a StringIndexer to convert the \"Gender\" column into numerical indices\n",
        "indexer = StringIndexer(inputCol=\"Gender\", outputCol=\"GenderIndex\")\n",
        "\n",
        "# Create a OneHotEncoder to convert the \"GenderIndex\" column into a binary vector representation\n",
        "encoder = OneHotEncoder(inputCols=[\"GenderIndex\"], outputCols=[\"GenderVec\"])\n",
        "\n",
        "# Update the VectorAssembler to include the \"GenderVec\" column in the inputCols\n",
        "vector_assembler = VectorAssembler(inputCols=[\"TotalCredit\", \"MonthlyPayments\", \"CreditScore\", \"CustomerAge\", \"GenderVec\", \"OwnsCar\", \"OwnsProperty\", \"NumberOfChildren\"], outputCol=\"features\")\n",
        "\n",
        "# Create a pipeline with the updated stages and Random Forest classifier\n",
        "pipeline_rf = Pipeline(stages=[indexer, encoder, vector_assembler, rf])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "model_rf = pipeline_rf.fit(training_data)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_rf = model_rf.transform(test_data)\n",
        "\n",
        "# Show the predicted and actual fraud labels\n",
        "predictions_rf.select(\"Fraud\", \"prediction\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5v6BLusmdSa",
        "outputId": "91966380-15ea-4163-c70c-aa1fcb53c81b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "|Fraud|prediction|\n",
            "+-----+----------+\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    1|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Create a BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Fraud\")\n",
        "\n",
        "# Calculate the AUC-ROC\n",
        "auc_roc = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderROC\"})\n",
        "print(f\"AUC-ROC: {auc_roc}\")\n",
        "\n",
        "# Calculate the AUC-PR\n",
        "auc_pr = evaluator.evaluate(predictions_rf, {evaluator.metricName: \"areaUnderPR\"})\n",
        "print(f\"AUC-PR: {auc_pr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgjQSrRunjko",
        "outputId": "8e186b82-4f84-4648-cf1e-6200e6c642e9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC-ROC: 0.9209713295914127\n",
            "AUC-PR: 0.45256803562764064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Create a multiclass classification evaluator\n",
        "evaluator_multiclass = MulticlassClassificationEvaluator(labelCol=\"Fraud\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = evaluator_multiclass.evaluate(predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YR3YS7wuU7u",
        "outputId": "1d7b4df0-f2e8-453f-ce64-bfbe6c608ca0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9154747487971722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "\n",
        "# Import necessary modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, count, sum, avg, max, min\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"FraudAnalysis\").getOrCreate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrHuEM_5twye",
        "outputId": "4c77d144-d990-4524-8ee0-ee75f7a2c5c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285397 sha256=48c188ba33e26268516f444d80da408f80aaa17d2cf76ee074299fa806383ad3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Excel data from GitHub URL\n",
        "excel_file_url = \"https://github.com/Ramme121/project/raw/main/FraudData.xlsx\"\n",
        "df_pandas = pd.read_excel(excel_file_url)\n",
        "\n",
        "# Convert the Pandas DataFrame to a PySpark DataFrame\n",
        "df = spark.createDataFrame(df_pandas)\n",
        "\n",
        "# Data Preprocessing\n",
        "# Assuming you have already done the data preprocessing, let's proceed with the analysis\n"
      ],
      "metadata": {
        "id": "WJP3snUgw8mn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Average LatePaymentAmount For Fraud and Non-Frauds Using SQL\n",
        "df.createOrReplaceTempView(\"FraudData\")\n",
        "late_payment_fraud_analysis = spark.sql(\n",
        "    \"SELECT Fraud, AVG(LatePaymentAmount) AS AverageLatePaymentAmount FROM FraudData GROUP BY Fraud\"\n",
        ")\n",
        "\n",
        "# Show the results for Late Payment Fraud Analysis\n",
        "late_payment_fraud_analysis.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSqUdXCiyQcN",
        "outputId": "92f95499-19c5-498d-e0bc-c6a2232148ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------------+\n",
            "|Fraud|AverageLatePaymentAmount|\n",
            "+-----+------------------------+\n",
            "|    0|       40983.30796360626|\n",
            "|    1|       59970.60809667674|\n",
            "+-----+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for classification\n",
        "assembler = VectorAssembler(inputCols=[\"CustomerAge\", \"LatePaymentAmount\"], outputCol=\"features\")\n",
        "\n",
        "# Create the Logistic Regression classifier\n",
        "lr = LogisticRegression(labelCol=\"Fraud\", featuresCol=\"features\", maxIter=10)\n",
        "\n",
        "# Create a Pipeline to chain the VectorAssembler and LogisticRegression\n",
        "pipeline = Pipeline(stages=[assembler, lr])\n",
        "\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
        "(training_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train the model using the Pipeline\n",
        "model = pipeline.fit(training_data)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate the model using BinaryClassificationEvaluator for both AUC and accuracy\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Fraud\", rawPredictionCol=\"rawPrediction\")\n",
        "au_roc = evaluator.setMetricName(\"areaUnderROC\").evaluate(predictions)\n",
        "au_pr = evaluator.setMetricName(\"areaUnderPR\").evaluate(predictions)\n",
        "\n",
        "# Create a DataFrame to store the evaluation results\n",
        "evaluation_result = spark.createDataFrame([(au_roc, \"Area Under ROC\"), (au_pr, \"Area Under PR\")], [\"Value\", \"Metric\"])\n",
        "\n",
        "# Show the evaluation results\n",
        "evaluation_result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uURfKdsLyjeQ",
        "outputId": "4dfd799e-2e22-4bdb-cf5a-b42ce4688e05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------+\n",
            "|              Value|        Metric|\n",
            "+-------------------+--------------+\n",
            "|  0.846113825639713|Area Under ROC|\n",
            "|0.28770872586642376| Area Under PR|\n",
            "+-------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "risk_level_analysis = df.withColumn(\"RiskLevel\", when(col(\"RiskLevel\").isNull(), \"No Risk\").otherwise(col(\"RiskLevel\"))) \\\n",
        "    .groupBy(\"RiskLevel\") \\\n",
        "    .agg(\n",
        "        count(\"LoanID\").alias(\"TotalLoans\"),\n",
        "        sum(\"Fraud\").alias(\"TotalFrauds\"),\n",
        "        avg(\"AnnualIncome\").alias(\"AvgIncome\"),\n",
        "        avg(\"CreditScore\").alias(\"AvgCreditScore\"),\n",
        "        max(\"CustomerAge\").alias(\"MaxCustomerAge\"),\n",
        "        min(\"CustomerAge\").alias(\"MinCustomerAge\")\n",
        "    )\n",
        "\n",
        "# Show the results for Risk Level Analysis\n",
        "risk_level_analysis.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K11rLKKy04x",
        "outputId": "77585323-a177-4f2a-9190-053cbbb6167e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+-----------+------------------+------------------+--------------+--------------+\n",
            "|RiskLevel|TotalLoans|TotalFrauds|         AvgIncome|    AvgCreditScore|MaxCustomerAge|MinCustomerAge|\n",
            "+---------+----------+-----------+------------------+------------------+--------------+--------------+\n",
            "|     None|    105786|          0| 169088.9987063978|399.64179570075436|            75|            30|\n",
            "|  LowRisk|     78841|       8242|169608.79358316105|372.61752134041933|            75|            25|\n",
            "| HighRisk|     43601|       8301| 167419.7960801358| 352.2618517923901|            75|            25|\n",
            "|     Risk|     79283|       8282|168361.07009068778| 373.3037725615832|            75|            25|\n",
            "+---------+----------+-----------+------------------+------------------+--------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "CsyHK7eoy866"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}